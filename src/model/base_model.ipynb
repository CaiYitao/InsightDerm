{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import convnext_large, ConvNeXt_Large_Weights\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "from PIL import Image\n",
    "import glob\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(2022)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNext = convnext_large(weights=ConvNeXt_Large_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LKA(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()        \n",
    "        attn = self.conv0(x)\n",
    "        attn = self.conv_spatial(attn)\n",
    "        attn = self.conv1(attn)\n",
    "\n",
    "        return u * attn\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.spatial_gating_unit = LKA(d_model)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shorcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        x = x + shorcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    classes = 3\n",
    "    att_dim = 1536\n",
    "    in_size = 1536\n",
    "    out_size = 3\n",
    "    dropout = 0.5\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InstanceClassifier(nn.Module):\n",
    "#     def __init__(self,config):\n",
    "#         super(InstanceClassifier, self).__init__()\n",
    "#         # self.FE = FeatureExtractor\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Linear(config.in_size, config.classes)\n",
    "#     def forward(self,features):\n",
    "#         # features = self.FE(x)\n",
    "\n",
    "#         h = self.avgpool(features)\n",
    "       \n",
    "#         feats = h.view(h.size(0), -1)\n",
    "    \n",
    "#         C = self.classifier(feats)\n",
    "\n",
    "#         return feats, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceClassifier(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(InstanceClassifier, self).__init__()\n",
    "  \n",
    "        self.classifier = nn.Linear(config.in_size, config.classes)\n",
    "    def forward(self,features):\n",
    "        # features = self.FE(x)\n",
    "\n",
    "   \n",
    "        C = self.classifier(features)\n",
    "\n",
    "        return features, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536, 7, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_att = Attention(1536) \n",
    "vis_att.to(device)\n",
    "vis_att(torch.randn(1,1536,7,7).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttDual(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(AttDual,self).__init__()\n",
    "        self.config = config\n",
    "   \n",
    "        self.i_classifier = InstanceClassifier(config)\n",
    "\n",
    "        self.key = nn.Sequential(nn.Linear(config.in_size, config.in_size),\n",
    "        nn.Dropout(config.dropout),\n",
    "        nn.LayerNorm(config.in_size),\n",
    "        nn.GELU())  #in_size=1536 after average pooling of features\n",
    "\n",
    "        self.query = nn.Sequential(nn.Linear(config.in_size, config.in_size),\n",
    "        nn.Dropout(config.dropout),\n",
    "        nn.LayerNorm(config.in_size),\n",
    "        nn.GELU())\n",
    "\n",
    "        self.value = nn.Sequential(nn.Linear(config.in_size, config.in_size),\n",
    "        nn.Dropout(config.dropout),\n",
    "        nn.LayerNorm(config.in_size),\n",
    "        nn.GELU())\n",
    "\n",
    "        self.head = nn.Conv1d(config.out_size, config.out_size, kernel_size=config.in_size)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "\n",
    "    def init_weights(self, m):\n",
    "       \n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "        elif isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self,features):\n",
    "     \n",
    "      \n",
    "        # print(f\"features shape after visual spatial attention: {features.shape}\")\n",
    "\n",
    "        features,c = self.i_classifier(features) #classifier output is features after pooling/reshape to (B,K) and instance classes\n",
    "\n",
    "        K = self.key(features)\n",
    "        # print(f\"Key after applying self.key: {K.shape}\")\n",
    "        V = self.value(K)  #B * K , unsorted\n",
    "        Q = self.query(K)# The QK(\"query-key\")circuits controls which features/tokens the head prefers to attend to.\n",
    "        # print(f\"Query after applying self.query to Key and no view(B,-1): {Q.shape}\")\n",
    "\n",
    "        # handle multiple classes without for loop\n",
    "        _, m_indices = torch.sort(c, 0, descending=True) # sort class scores along the instance dimension, m_indices in shape N x C\n",
    "        # print(f\"m_indices shape: {m_indices.shape}\")\n",
    "        m_feats = torch.index_select(K, dim=0, index=m_indices[0, :]) # select critical instances, m_feats in shape C x K \n",
    "        # print(f\"m_feats shape: {m_feats.shape}\")\n",
    "        q_max = self.query(m_feats) # compute queries of critical instances, q_max in shape C x Q\n",
    "        # print(f\"q_max shape: {q_max.shape}\")\n",
    "        A = torch.mm(Q, q_max.transpose(0, 1)) # compute inner product of Q to each entry of q_max, A in shape N x C, each column contains unnormalized attention scores\n",
    "        # print(f\"A score shape: {A.shape}\")\n",
    "        A = F.softmax( A / torch.sqrt(torch.tensor(Q.shape[1], dtype=torch.float32, device=device)), 0) # normalize attention scores, A in shape N x C, \n",
    "        # print(f\"A softmax normalized score shape: {A.shape}\")\n",
    "        B = torch.mm(A.transpose(0, 1), V) # compute bag representation, B in shape C x V\n",
    "        # print(f\"results after score multiply with Value shape: {B.shape}\")\n",
    "                \n",
    "        B = B.view(1, B.shape[0], B.shape[1]) # 1 x C x V\n",
    "        # print(f\"B unsqueezed shape: {B.shape}\")\n",
    "        C = self.head(B) # 1 x C x 1\n",
    "        # print(f\"Class shape after 1 d convolution: {C.shape}\")\n",
    "        C = C.view(1, -1)\n",
    "        # print(f\"Class shape after reshape: {C.shape}\")\n",
    "        return C, A, B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_att = Attention(config.att_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisAttDual(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(VisAttDual,self).__init__()\n",
    "        self.config = config\n",
    "        self.vis_att = Attention(config.att_dim)   #visual att_dim features channel size 1536\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.i_classifier = InstanceClassifier(config)\n",
    "\n",
    "        self.key = nn.Sequential(nn.Linear(config.in_size, config.in_size),\n",
    "        nn.Dropout(config.dropout),\n",
    "        nn.LayerNorm(config.in_size),\n",
    "        nn.GELU())  #in_size=1536 after average pooling of features\n",
    "\n",
    "        self.query = nn.Sequential(nn.Linear(config.in_size, config.in_size),\n",
    "        nn.Dropout(config.dropout),\n",
    "        nn.LayerNorm(config.in_size),\n",
    "        nn.GELU())\n",
    "\n",
    "        self.value = nn.Sequential(nn.Linear(config.in_size, config.in_size),\n",
    "        nn.Dropout(config.dropout),\n",
    "        nn.LayerNorm(config.in_size),\n",
    "        nn.GELU())\n",
    "\n",
    "        self.head = nn.Conv1d(config.out_size, config.out_size, kernel_size=config.in_size)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "\n",
    "    def init_weights(self, m):\n",
    "       \n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "        elif isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self,features):\n",
    "        features = features.view(features.size(0), 1536,7,7)\n",
    "        features = self.vis_att(features) #apply visual spactial attention to features\n",
    "        # print(f\"features shape after visual spatial attention: {features.shape}\")\n",
    "\n",
    "        features,c = self.i_classifier(features) #classifier output is features after pooling/reshape to (B,K) and instance classes\n",
    "\n",
    "        K = self.key(features)\n",
    "        # print(f\"Key after applying self.key: {K.shape}\")\n",
    "        V = self.value(K)  #B * K , unsorted\n",
    "        Q = self.query(K)# The QK(\"query-key\")circuits controls which features/tokens the head prefers to attend to.\n",
    "        # print(f\"Query after applying self.query to Key and no view(B,-1): {Q.shape}\")\n",
    "\n",
    "        # handle multiple classes without for loop\n",
    "        _, m_indices = torch.sort(c, 0, descending=True) # sort class scores along the instance dimension, m_indices in shape N x C\n",
    "        # print(f\"m_indices shape: {m_indices.shape}\")\n",
    "        m_feats = torch.index_select(K, dim=0, index=m_indices[0, :]) # select critical instances, m_feats in shape C x K \n",
    "        # print(f\"m_feats shape: {m_feats.shape}\")\n",
    "        q_max = self.query(m_feats) # compute queries of critical instances, q_max in shape C x Q\n",
    "        # print(f\"q_max shape: {q_max.shape}\")\n",
    "        A = torch.mm(Q, q_max.transpose(0, 1)) # compute inner product of Q to each entry of q_max, A in shape N x C, each column contains unnormalized attention scores\n",
    "        # print(f\"A score shape: {A.shape}\")\n",
    "        A = F.softmax( A / torch.sqrt(torch.tensor(Q.shape[1], dtype=torch.float32, device=device)), 0) # normalize attention scores, A in shape N x C, \n",
    "        # print(f\"A softmax normalized score shape: {A.shape}\")\n",
    "        B = torch.mm(A.transpose(0, 1), V) # compute bag representation, B in shape C x V\n",
    "        # print(f\"results after score multiply with Value shape: {B.shape}\")\n",
    "                \n",
    "        B = B.view(1, B.shape[0], B.shape[1]) # 1 x C x V\n",
    "        # print(f\"B unsqueezed shape: {B.shape}\")\n",
    "        C = self.head(B) # 1 x C x 1\n",
    "        # print(f\"Class shape after 1 d convolution: {C.shape}\")\n",
    "        C = C.view(1, -1)\n",
    "        # print(f\"Class shape after reshape: {C.shape}\")\n",
    "        return C, A, B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSMILNet(nn.Module):\n",
    "    def __init__(self, i_classifier, b_classifier):\n",
    "        super(DSMILNet, self).__init__()\n",
    "        self.i_classifier = i_classifier\n",
    "        self.b_classifier = b_classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats, classes = self.i_classifier(x)\n",
    "        prediction_bag, A, B = self.b_classifier(feats, classes)\n",
    "        \n",
    "        return classes, prediction_bag, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 20).reshape(4,5)[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag class tensor([[ 0.1915, -0.5536, -0.6501]], device='cuda:0', grad_fn=<ViewBackward0>) shape: torch.Size([1, 3])\n",
      "Bag attention shape: torch.Size([128, 3])\n",
      "b shape: torch.Size([1, 3, 1536])\n"
     ]
    }
   ],
   "source": [
    "device = torch.cuda.current_device()\n",
    "config = ModelConfig(classes=3, att_dim=1536, in_size=1536, out_size=3)\n",
    "f = torch.rand(128,1536 * 7*7).to(device)\n",
    "\n",
    "m = VisAttDual(config).to(device)\n",
    "c,a,b = m(f)\n",
    "print(f\"Bag class {c} shape: {c.shape}\")\n",
    "print(f\"Bag attention shape: {a.shape}\")\n",
    "print(f\"b shape: {b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisAtt(nn.Module):\n",
    "    def __init__(self, num_classes: int, repr_length = 1536, dimension = 1000, att=1,dropout=0.5):\n",
    "        super(VisAtt, self).__init__()\n",
    "        # self.features = features\n",
    "        self.vis_att = Attention(dimension)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.method = method\n",
    "   \n",
    "        self.repr_length = repr_length # size of representation per tile\n",
    "        self.D = dimension                               # N = batch size\n",
    "        self.att = att\n",
    "\n",
    "        self.attention = nn.Sequential(             # N * repr_length\n",
    "            nn.Linear(self.repr_length, self.D),    # N * D\n",
    "            nn.LayerNorm(self.D, eps=1e-06, elementwise_affine=True),\n",
    "            nn.GELU(),                              # N * D\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.D, self.att)             # N * att\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.repr_length*self.att, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention.apply(self._init_weights)\n",
    "        self.classifier.apply(self._init_weights)\n",
    "        self.vis_att.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "       \n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self,features):\n",
    "\n",
    "        features = features.view(features.size(0), 1536,7,7)    # N * repr_length * 7 * 7  size after ConvNext.features(batched tiles)\n",
    "\n",
    "        h = self.vis_att(features)\n",
    "        h = self.avgpool(h)\n",
    "       \n",
    "        h = h.view(h.size(0), -1)\n",
    "\n",
    "        A = self.attention(h)       # N * att\n",
    "  \n",
    "        A = torch.transpose(A, 1,0)                     # att * N\n",
    "     \n",
    "        A = F.softmax(A, dim=1)                         # softmax over N\n",
    "  \n",
    "        M = torch.mm(A, h)   # att * repr_length\n",
    "        out_prob = self.classifier(M)\n",
    "\n",
    "        y_hat = torch.ge(out_prob, 0.5).float()\n",
    "        return out_prob, A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 75264])\n",
      "torch.Size([10, 1536, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4681, 0.4026, 0.5655]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.0187, 0.0402, 0.0429, 0.0156, 0.0741, 0.3673, 0.0907, 0.0041, 0.1246,\n",
       "          0.2217]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f.shape)\n",
    "fv = f.view(f.size(0), 1536, 7, 7)\n",
    "print(fv.shape)\n",
    "m = VisAtt(3)\n",
    "m(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133ff1855f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15b815a470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16dbf55509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16f7d9dbc2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181ec92105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>11fffcc1f6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ICNNBCC00362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>019f23d6b7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ICNNBCC00386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>318027c81c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  1\n",
       "0      133ff1855f  1\n",
       "1      15b815a470  1\n",
       "2      16dbf55509  1\n",
       "3      16f7d9dbc2  1\n",
       "4      181ec92105  1\n",
       "..            ... ..\n",
       "127    11fffcc1f6  1\n",
       "128  ICNNBCC00362  0\n",
       "129    019f23d6b7  1\n",
       "130  ICNNBCC00386  0\n",
       "131    318027c81c  1\n",
       "\n",
       "[132 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# label_file_train = \"/system/user/kimesweg/data/small/metadata_train_new.csv\"\n",
    "# label_file_test = \"/system/user/kimesweg/data/small/metadata_test_new.csv\"\n",
    "# labels_test = pd.read_csv(label_file_test,index_col=0)\n",
    "# labels_train = pd.read_csv(label_file_train,index_col=0)\n",
    "# labels = pd.concat((labels_test, labels_train))\n",
    "# Data_test = pd.read_csv(label_file_test)\n",
    "# Data_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b02ae7d93fc336b7dd4a84931e3457dcb2360f1e08d8d05071825acaef6f3b22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
